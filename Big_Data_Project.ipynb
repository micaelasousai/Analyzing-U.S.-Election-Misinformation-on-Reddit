{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micaelasousai/Analyzing-U.S.-Election-Misinformation-on-Reddit/blob/main/Big_Data_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4X0uydHfW8Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWTp2SSFgOSN"
      },
      "source": [
        "# **Project: Engagement Metrics - Do False Claims Spread Faster?**\n",
        "## **Objective**\n",
        "This project aims to analyze how misinformation spreads on Reddit during the 2024 U.S. Presidential election. Specifically, we will:\n",
        "- Collect election-related discussions from Reddit.\n",
        "- Use NLP techniques to classify misinformation.\n",
        "- Compare engagement levels (comments, virality, shares) between fact-checked and misleading posts.\n",
        "- Visualize the spread of misinformation vs. factual content.\n",
        "\n",
        "## **Tools & Libraries**\n",
        "- **Programming Languages**: Python\n",
        "- **APIs**: Reddit API (PRAW)\n",
        "- **Libraries**: NLTK, Transformers, PRAW, Pandas, Matplotlib, Seaborn\n",
        "- **Visualization**: Matplotlib\n",
        "- **Data Storage**: Google Drive / Local Storage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wgs_OEiyghxD"
      },
      "source": [
        "## **Stage 1: Collecting Data from Reddit**\n",
        "We used PRAW to fetch data from relevant subreddits like r/politics, r/news, and r/2024elections, then organized the posts and comments into a structured Pandas DataFrame for analysis.\n",
        "\n",
        "**The block of code below takes approximately 70 minutes to run so we recommend that you skip testing it and use the reddit_scrape.csv file attached for the steps following it**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEDgjhdJgcfZ"
      },
      "outputs": [],
      "source": [
        "!pip install praw\n",
        "\n",
        "import praw\n",
        "import time\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pprint import pprint\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "logging.getLogger(\"praw\").setLevel(logging.ERROR)\n",
        "\n",
        "start_date = datetime(2023, 6, 1)\n",
        "end_date = datetime(2024, 12, 31)\n",
        "start_ts = start_date.timestamp()\n",
        "end_ts = end_date.timestamp()\n",
        "\n",
        "user_agent = \"Scraper 1.0 by /u/SouthBee4571\"\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"dfKJovGqVILyM2BBEmUb5w\",\n",
        "    client_secret=\"XTmfF9JE6Z4kkGPBPTuJJGP2SRnrnw\",\n",
        "    user_agent=user_agent\n",
        ")\n",
        "\n",
        "subreddits = [\n",
        "    'politics',\n",
        "    'news',\n",
        "    'Conservative',\n",
        "    'liberal',\n",
        "    'PoliticalDiscussion',\n",
        "    '2024elections'\n",
        "]\n",
        "\n",
        "posts_data = []\n",
        "\n",
        "for sub in subreddits:\n",
        "    for submission in reddit.subreddit(sub).hot(limit=None):\n",
        "        if not (start_ts <= submission.created_utc <= end_ts):\n",
        "            continue\n",
        "        time.sleep(3)\n",
        "        comments = [comment.body for comment in submission.comments if isinstance(comment, praw.models.Comment)][:30]\n",
        "\n",
        "        score = submission.score\n",
        "        upvote_ratio = submission.upvote_ratio\n",
        "        estimated_upvotes = int(score * upvote_ratio)\n",
        "        estimated_downvotes = score - estimated_upvotes\n",
        "\n",
        "        posts_data.append({\n",
        "            \"post_id\": submission.id,\n",
        "            \"title\": submission.title,\n",
        "            \"selftext\": submission.selftext,\n",
        "            \"url\": submission.url,\n",
        "            \"comments\": comments,\n",
        "            \"num_comments\": submission.num_comments,\n",
        "            \"upvotes\": estimated_upvotes,\n",
        "            \"downvotes\": estimated_downvotes,\n",
        "            \"score\": score,\n",
        "            \"timestamp\": submission.created_utc,\n",
        "            \"subreddit\": submission.subreddit.display_name\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(posts_data)\n",
        "df['datetime'] = pd.to_datetime(df['timestamp'], unit='s').astype(str)\n",
        "df['content'] = df.apply(\n",
        "    lambda row: row['title'] + \"\\n\\n\" + (row['selftext'] if row['selftext'] else \"\") + \"\\n\" + row['url'],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(df[['subreddit', 'datetime', 'title']].head())\n",
        "\n",
        "df.to_csv(\"reddit_scrape.csv\", index=False, encoding='utf-8')\n",
        "files.download(\"reddit_scrape.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEvCm4nSgpf1"
      },
      "source": [
        "## **Stage 2: Cleaning and Preprocessing Data**\n",
        "\n",
        "We cleaned the text data by removing stopwords and special characters from the title and self-text fields, converting everything to lowercase, and tokenizing the words using NLTK, then stored the cleaned results in a new DataFrame for further analysis in the stages following.\n",
        "\n",
        "**In the code cell below, upload the reddit_scrape.csv file when testing**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the reddit_scrape.csv file here\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n"
      ],
      "metadata": {
        "id": "megfyGJxWQRM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "17d1e60d-daba-46c5-e0ae-77db0ede636e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f3e69647-3dd8-49f5-a9af-2956d5273e30\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f3e69647-3dd8-49f5-a9af-2956d5273e30\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f5ec84e6aa6f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Upload the reddit_scrape.csv file here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HntckPOUgsff"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt_tab', force=True)\n",
        "nltk.download('stopwords', force=True)\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Read CSV file into a DataFrame\n",
        "df = pd.read_csv(\"reddit_scrape.csv\", encoding='utf-8')\n",
        "\n",
        "# Apply cleaning function to 'title' and 'selftext' columns\n",
        "df[\"Cleaned_Title\"] = df[\"title\"].apply(lambda x: clean_text(str(x)))\n",
        "df[\"Cleaned_Selftext\"] = df[\"selftext\"].apply(lambda x: clean_text(str(x)))\n",
        "\n",
        "# View the cleaned content\n",
        "print(df[['title', 'Cleaned_Title', 'selftext', 'Cleaned_Selftext']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N4JLb43g2Ns"
      },
      "source": [
        "## **Stage 3: Fact-Checking Misinformation**\n",
        "\n",
        "We trained a machine learning model using a labeled dataset from Kaggle to evaluate the accuracy of news content. The model was designed to classify each post along a six-point scale ranging from “true” (1) to “completely_false” (6), allowing us to assess the credibility of Reddit posts with more nuance. These labels helped us determine the degree of misinformation in each post rather than simply classifying content as either verified or misleading."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the test.csv file here\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "x0RfnkwM7hPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QxkJ9g-g0DY"
      },
      "outputs": [],
      "source": [
        "#Creating an ML algorithm to detect fake news from headlines\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "#adding the csv file that will be used to train the ML with fake and real news\n",
        "#training_df = '/content/drive/My Drive/Reddit_folder/test.csv' # this csv are from my personal drive. Data is from: https://www.kaggle.com/datasets/arashnic/fake-claim-dataset/data\n",
        "\n",
        "training = pd.read_csv(\"test.csv\")\n",
        "\n",
        "#adding the range to the labels in the training dataset (This range label is from Kraggle's)\n",
        "label_range = {\n",
        "    \"true\": 1,\n",
        "    \"mostly-true\": 2,\n",
        "    \"half-true\": 3,\n",
        "    \"barely-true\": 4,\n",
        "    \"false\": 5,\n",
        "    \"pants-fire\": 6\n",
        "}\n",
        "\n",
        "training['label'] = training['label'].map(label_range)\n",
        "\n",
        "training = training.dropna(subset=['label'])#dropping rows with Na\n",
        "\n",
        "# splitting dataset into features and labels\n",
        "X = training['statement']  # using the text to train the ML\n",
        "y = training['label']\n",
        "\n",
        "# splitting data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# using TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# log regression model for training\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', class_weight='balanced', max_iter=1000)\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "#will use text instead of redit title to check for fake and real news\n",
        "df[\"Cleaned_Selftext\"] = df[\"selftext\"].apply(lambda x: clean_text(str(x)))\n",
        "reddit_titles = df['selftext'] #getting the reddit titles\n",
        "\n",
        "# using the trained TF-IDF Vectorizer\n",
        "reddit_titles_tfidf = vectorizer.transform(df[\"Cleaned_Selftext\"])\n",
        "\n",
        "# predicting whether the reddit titles are fake or real news\n",
        "reddit_predictions = model.predict(reddit_titles_tfidf)\n",
        "\n",
        "#adding our own range names:\n",
        "inverse_label_range = {\n",
        "    1: \"true\",\n",
        "    2: \"mostly-true\",\n",
        "    3: \"half-true\",\n",
        "    4: \"barely-true\",\n",
        "    5: \"false\",\n",
        "    6: \"completely-false\"\n",
        "}\n",
        "\n",
        "# adding prediction labels\n",
        "df['prediction'] = reddit_predictions\n",
        "df['prediction'] = df['prediction'].map(inverse_label_range)\n",
        "\n",
        "\n",
        "#printing the first few to check\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Tkwz5EhYLiAT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEwHwlBwgvHP"
      },
      "source": [
        "## **Stage 4: Classify Posts Using Sentiment Analysis**\n",
        "We used NLTK for sentiment analysis to evaluate the tone of each post, classifying them as positive, neutral, or negative based on their content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5qwQsl5g5er"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "!pip install nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Initialize sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Sentiment function\n",
        "def get_sentiment(post):\n",
        "  scores = analyzer.polarity_scores(post)\n",
        "  compound = scores['compound']\n",
        "  if compound >= 0.05:\n",
        "    sentiment = 'positive'\n",
        "  elif compound <= -0.05:\n",
        "    sentiment = 'negative'\n",
        "  else:\n",
        "    sentiment = 'neutral'\n",
        "  return sentiment\n",
        "\n",
        "# Apply function to get sentiment\n",
        "df['Sentiment'] = df['Cleaned_Selftext'].apply(get_sentiment)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHJ0gp2Xg_6k"
      },
      "source": [
        "## **Stage 5: Analyze Engagement Metrics**\n",
        "We computed various engagement metrics such as comment activity, virality score, and controversy ratio to understand how users interact with different types of content. These metrics were then compared between factual and misleading posts to assess whether false claims tend to spread faster. Additionally, we analyzed and compared the average sentiment of factual posts versus misinformation to identify notable differences in tone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjM1JprnrLAX"
      },
      "source": [
        "**Comment Activity**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bAJdmi1hAgZ"
      },
      "outputs": [],
      "source": [
        "#convert num_comments to numeric\n",
        "df['num_comments'] = pd.to_numeric(df['num_comments'], errors='coerce')\n",
        "df['Comment_Activity'] = df['num_comments']\n",
        "#print the results\n",
        "print(\"Average number of comments per post:\", df['Comment_Activity'].mean())\n",
        "print(\"Top 5 most commented posts:\")\n",
        "top5 = df[['title', 'Comment_Activity']].sort_values(by='Comment_Activity', ascending=False).head().reset_index(drop=True)\n",
        "print(top5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYYlSe9irYHC"
      },
      "source": [
        "**Virality Score**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3WtdCWlrdzy"
      },
      "outputs": [],
      "source": [
        "# Code Section for the above\n",
        "import time\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Convert timestamp to datetime\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
        "\n",
        "# Calculate post age in hours (how long it's been online)\n",
        "df['post_age_hours'] = (datetime.utcnow() - df['timestamp']).dt.total_seconds() / 3600\n",
        "\n",
        "# Set weights\n",
        "W1 = 2  # Weight for crossposts\n",
        "W2 = 1  # To prevent division by zero\n",
        "\n",
        "# Ensure timestamp column is in datetime format\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "\n",
        "# Convert timestamps to Unix time (seconds since epoch)\n",
        "df['timestamp'] = df['timestamp'].astype('int64') // 10**9  # Convert to seconds\n",
        "\n",
        "current_time = time.time()\n",
        "df['post_age_hours'] = (current_time - df['timestamp']) / 3600\n",
        "\n",
        "# Calculate virality score\n",
        "df['virality_score'] = (df['upvotes'] + (df['num_comments'] * W1)) / (df['post_age_hours'] + W2)\n",
        "\n",
        "# Display top viral posts\n",
        "df[['post_id', 'title', 'virality_score']].sort_values(by='virality_score', ascending=False).head(10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxBec3ZWrmVH"
      },
      "source": [
        "**Controversy Ratio**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPYlwnylr4OT"
      },
      "outputs": [],
      "source": [
        "#making rows nan that are not ints in the upvotes and downvotes\n",
        "df['upvotes'] = pd.to_numeric(df['upvotes'], errors='coerce')\n",
        "df['downvotes'] = pd.to_numeric(df['downvotes'], errors='coerce')\n",
        "\n",
        "#deleting the nan rows\n",
        "df = df.dropna(subset = ['upvotes', 'downvotes'])\n",
        "\n",
        "#getting the upvotes + downvotes to check the ratio\n",
        "df['controversy_ratio'] = df['upvotes']/(df['downvotes'] + 1)\n",
        "\n",
        "#checking the first few\n",
        "print(df.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNMn-uJlr_-l"
      },
      "source": [
        "**Cross-Subredit Spread**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8fM5925sQ_4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create a unique identifier for posts based on title and selftext\n",
        "df['combined_text'] = df['Cleaned_Title'].fillna('') + ' ' + df['Cleaned_Selftext'].fillna('')\n",
        "\n",
        "# Vectorize the text using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(df['combined_text'])\n",
        "\n",
        "# Compute cosine similarity between posts\n",
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "# Set a similarity threshold (e.g., 0.7) to consider posts as near-duplicates\n",
        "similarity_threshold = 0.7\n",
        "\n",
        "# Identify cross-subreddit misinformation spread\n",
        "subreddit_spread = {}\n",
        "for i in range(len(df)):\n",
        "    similar_subreddits = set()\n",
        "    for j in range(len(df)):\n",
        "        if i != j and cosine_sim[i, j] > similarity_threshold:\n",
        "            similar_subreddits.add(df.iloc[j]['subreddit'])\n",
        "    subreddit_spread[df.iloc[i]['post_id']] = len(similar_subreddits)\n",
        "\n",
        "# Convert to DataFrame\n",
        "spread_df = pd.DataFrame(list(subreddit_spread.items()), columns=['post_id', 'unique_subreddit_count'])\n",
        "\n",
        "# Calculate Cross-Subreddit Spread Score\n",
        "# The Cross subreddit score is calculated as number of unique subreddits with similar posts divided by total number of subreddits in dataset\n",
        "spread_df['cross_subreddit_score'] = spread_df['unique_subreddit_count'] / df['subreddit'].nunique()\n",
        "\n",
        "average_score_true = spread_df[df['prediction'] == \"true\"]['cross_subreddit_score'].mean()\n",
        "average_score_mostly_true = spread_df[df['prediction'] == \"mostly-true\"]['cross_subreddit_score'].mean()\n",
        "average_score_half_true = spread_df[df['prediction'] == \"half-true\"]['cross_subreddit_score'].mean()\n",
        "average_score_barely_true = spread_df[df['prediction'] == \"barely-true\"]['cross_subreddit_score'].mean()\n",
        "average_score_false = spread_df[df['prediction'] == \"false\"]['cross_subreddit_score'].mean()\n",
        "average_score_completely_false = spread_df[df['prediction'] == \"completely-false\"]['cross_subreddit_score'].mean()\n",
        "\n",
        "print(\"Average Cross-Subreddit Spread Score for Posts with True Information:\", average_score_true)\n",
        "print(\"Average Cross-Subreddit Spread Score for Posts with Mostly-True Information:\", average_score_mostly_true)\n",
        "print(\"Average Cross-Subreddit Spread Score for Posts with Half-True Information:\", average_score_half_true)\n",
        "print(\"Average Cross-Subreddit Spread Score for Posts with Barely-True Information:\", average_score_barely_true)\n",
        "print(\"Average Cross-Subreddit Spread Score for Posts with False Information:\", average_score_false)\n",
        "print(\"Average Cross-Subreddit Spread Score for Posts with Completely-False Information:\", average_score_completely_false)\n",
        "\n",
        "# Save results\n",
        "spread_df.to_csv('cross_subreddit_spread.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ824vmxhLJV"
      },
      "source": [
        "## **Step 6: Visualize Findings & Build Dashboard**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if df['datetime'].dtype != 'datetime64[ns]':\n",
        "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "\n",
        "df = df[df['post_id'] != '1bwbuka']\n",
        "\n",
        "start_date = pd.to_datetime(\"2023-06-01\")\n",
        "end_date = pd.to_datetime(\"2024-12-31\")\n",
        "df_filtered = df[(df['datetime'] >= start_date) & (df['datetime'] <= end_date)].copy()\n",
        "\n",
        "df_filtered['date'] = df_filtered['datetime'].dt.date\n",
        "daily_activity = df_filtered.groupby('date')['Comment_Activity'].mean().reset_index()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.lineplot(data=daily_activity, x='date', y='Comment_Activity')\n",
        "plt.title(\"Figure 1. Daily Average Comment Activity (August – December 2024)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Avg num of comments\")\n",
        "plt.xlim([pd.to_datetime(\"2024-08-01\"), pd.to_datetime(\"2024-12-31\")])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AgcrbTlI6pwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Average Comment Activity**"
      ],
      "metadata": {
        "id": "GMA5i-PqY4tn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8AcJQRphLvZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df['Comment_Activity'] = df['num_comments']\n",
        "\n",
        "heatmap_data = df.pivot_table(\n",
        "    index='subreddit',\n",
        "    columns='prediction',\n",
        "    values='Comment_Activity',\n",
        "    aggfunc='mean'\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(heatmap_data, annot=True, fmt=\".1f\", cmap=\"YlGnBu\", linewidths=0.5)\n",
        "\n",
        "plt.title(\"Figure 2. Average Comment Activity by Subreddit and Verdict\")\n",
        "plt.xlabel(\"Predicted Verdict\")\n",
        "plt.ylabel(\"Subreddit\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Visualize Virality Score for Verified Versus Misleading Data**"
      ],
      "metadata": {
        "id": "5ZGqweptS3oL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate mean virality score per fact-check label\n",
        "avg_virality = df.groupby(\"prediction\")[\"virality_score\"].mean()\n",
        "\n",
        "# Set wider figure size\n",
        "plt.figure(figsize=(10, 6))  # You can tweak the width (10) as needed\n",
        "\n",
        "# Create bar chart\n",
        "sns.barplot(x=avg_virality.index, y=avg_virality.values, palette=[\"green\", \"red\"])\n",
        "plt.xlabel(\"Fact-Check Label\")\n",
        "plt.ylabel(\"Average Virality Score\")\n",
        "plt.title(\"Average Virality Score of Predicted Posts\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rh25nbP2S2r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graphing the controversy ratio**"
      ],
      "metadata": {
        "id": "ylSXrU4Oojru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "#will graph the controvery ratio based on misleading vs verified posts\n",
        "\n",
        "df['log_controversy_ratio'] = np.log1p(df['controversy_ratio']) #logging the controversy ratio to handle outliers\n",
        "\n",
        "#making the order of the x-axis (the misleading vs truthful news) to be in order\n",
        "in_order = [\"completely-false\", \"false\", \"barely-true\", \"half-true\", \"mostly-true\", \"true\"]\n",
        "\n",
        "sns.boxplot(x='prediction', y = 'log_controversy_ratio', data=df, palette=\"Set3\", order=in_order)\n",
        "plt.title(\"Controversy Ratio of Misleading vs Truthful News\")\n",
        "plt.xlabel(\"Fact Check Label\")\n",
        "plt.ylabel(\"Average controversy ratio\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jTQgg1Q8onDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross-Subredit Spread Table With Results**"
      ],
      "metadata": {
        "id": "Cq5ewQiXQhcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "# Create table to store the values\n",
        "cross_subreddit_spread_table = PrettyTable(['Post Accuracy Prediction', 'Average Cross-Subreddit Spread Score'])\n",
        "\n",
        "# Add rows to the table\n",
        "cross_subreddit_spread_table.add_row(['true', average_score_true])\n",
        "cross_subreddit_spread_table.add_row(['mostly-true', average_score_mostly_true])\n",
        "cross_subreddit_spread_table.add_row(['half-true', average_score_half_true])\n",
        "cross_subreddit_spread_table.add_row(['barely-true', average_score_barely_true])\n",
        "cross_subreddit_spread_table.add_row(['false', average_score_false])\n",
        "cross_subreddit_spread_table.add_row(['completely-false', average_score_completely_false])\n",
        "\n",
        "# Print the table with results\n",
        "print(cross_subreddit_spread_table)"
      ],
      "metadata": {
        "id": "OM4HRtUWRCna",
        "outputId": "bddced5a-9972-4586-dbc7-a3ecb20b6559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'average_score_barely_true' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-f67e63851379>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcross_subreddit_spread_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mostly-true'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_score_mostly_true\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mcross_subreddit_spread_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'half-true'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_score_half_true\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mcross_subreddit_spread_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'barely-true'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_score_barely_true\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mcross_subreddit_spread_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'false'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_score_false\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mcross_subreddit_spread_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'completely-false'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_score_completely_false\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'average_score_barely_true' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}